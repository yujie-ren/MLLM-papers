## Multimodal Large Language Model Papers
Search Keywords: Vision, Visual, Multimodal

## Contents
- 2025
  - [Other](#other)
  - [ICLR-202504](#iclr-202504)
  - [NAACL-202504](#naacl-202504)

## Other


## ICLR-202504
[All Papers](https://openreview.net/group?id=ICLR.cc/2025/Conference#tab-accept-oral) &nbsp;&nbsp;
[Multimodal Model Papers](https://iclr2025.vizhub.ai/?brushed=%255B%255B179.62503051757812%252C18.363710403442383%255D%252C%255B330.3000183105469%252C234.6387176513672%255D%255D)

####  VLM
1. Mechanistic Interpretability Meets Vision Language Models: Insights and Limitations  [[pdf]](https://d2jud02ci9yv69.cloudfront.net/2025-04-28-vlm-understanding-29/blog/vlm-understanding/)  
(Blog, THU, 全面介绍了Interpretability & VLM)
2. Fine-Tuning Token-Based Large Multimodal Models: What Works, What Doesn’t and What's Next  [[pdf]](https://d2jud02ci9yv69.cloudfront.net/2025-04-28-fine-tuning-token-based-large-multimodal-models-86/blog/fine-tuning-token-based-large-multimodal-models/)  
(Blog, Pengfei Liu from SHJT, 探索了VLM Fine-tuning)
3. Inference Optimal VLMs Need Fewer Visual Tokens and More Parameters  [[pdf]](https://openreview.net/forum?id=6VhDQP7WGX)  
(CMU, VLM, 研究了减少visual token和减少LLM参数的trade-off, 没有对比实验。)
4. Dynamic Multimodal Evaluation with Flexible Complexity by Vision-Language Bootstrapping  [[pdf]](https://openreview.net/forum?id=X1OfiRYCLn)  
(SJTU, VLM, VLM evaluation, data contamination)  
5. MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models  [[pdf]](https://openreview.net/forum?id=HnhNRrLPwm)  
(UNC-Chapel Hill, VLM, VLM evaluation)  
6. Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference under Ambiguities  [[pdf]](https://openreview.net/forum?id=84pDoCD4lH)  
(U of Michigan, VLM evaluation)  
7. PhysBench: Benchmarking and Enhancing Vision-Language Models for Physical World Understanding  [[pdf]](https://openreview.net/forum?id=Q6a9W6kzv5)  
(USC, VLM Evaluation)  
8.  Reducing Hallucinations in Large Vision-Language Models via Latent Space Steering  [[pdf]](https://openreview.net/forum?id=LBl7Hez0fF)  
(Stanford, VLM Hallucination)  
9. Vision Language Models are In-Context Value Learners  [[pdf]](https://openreview.net/forum?id=friHAl5ofG)  
(Google, VLM, Robots)  
10. AHA: A Vision-Language-Model for Detecting and Reasoning Over Failures in Robotic Manipulation  [[pdf]](https://openreview.net/forum?id=JVkdSi7Ekg)  
(Nvidia, VLM, Robot, Detect failures)  
11. Teaching Human Behavior Improves Content Understanding Abilities Of VLMs  [[pdf]](https://openreview.net/forum?id=ff2V3UR9sC)  
(Adobe, VLM, )


#### Other
1. Knowledge Entropy Decay during Language Model Pretraining Hinders New Knowledge Acquisition  [[pdf]](https://openreview.net/forum?id=eHehzSDUFp)  
(KAIST AI, Pretraining过程中参数化知识的整合倾向)
2.  Context-Parametric Inversion: Why Instruction Finetuning May Not Actually Improve Context Reliance  [[pdf]](https://openreview.net/forum?id=SPS6HzVzyt)  
(CMU，知识冲突中的上下文依赖)
3.  Scaling LLM Test-Time Compute Optimally Can be More Effective than Scaling Parameters for Reasoning  [[pdf]](https://openreview.net/forum?id=4FWAwZtd2n)  
(UC Berkeley，test time computing)
4.  Capturing the Temporal Dependence of Training Data Influence  [[pdf]](https://openreview.net/forum?id=uHLgDEgiS5)  
(Princeton, the influence of specific training data)
5. Data Shapley in One Training Run  [[pdf]](https://openreview.net/forum?id=HD6bWcj87Y)  
(Princeton, the influence of specific training data)
6.  Interpreting Emergent Planning in Model-Free Reinforcement Learning  [[pdf]](https://openreview.net/forum?id=DzGe40glxs)  
(Cambridge, Interpretability, RL agents)  
7.  Unlocking the Power of Function Vectors for Characterizing and Mitigating Catastrophic Forgetting in Continual Instruction Tuning  [[pdf]](https://openreview.net/forum?id=gc8QAQfXv6)  
(USTC, Catastrophic forgetting, Continual learning)  
8. Two Effects, One Trigger: On the Modality Gap, Object Bias, and Information Imbalance in Contrastive Vision-Language Models  [[pdf]](https://openreview.net/forum?id=uAFHCZRmXk)  
(U of Freiburg, CLIP)  
9. Weighted Point Set Embedding for Multimodal Contrastive Learning Toward Optimal Similarity Metric  [[pdf]](https://openreview.net/forum?id=uSz2K30RRd)  
(Sony AI, CLIP)  
10. VLMaterial: Procedural Material Generation with Large Vision-Language Models  [[pdf]](https://openreview.net/forum?id=wHebuIb6IH)  
(MIT, VLM, Computer graphics)  
11. LLaVA-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models  [[pdf]](https://openreview.net/forum?id=oSQiao9GqB)  
(ByteDance, VLM, Multi-image, Video, 3D)  
12. Large-scale and Fine-grained Vision-language Pre-training for Enhanced CT Image Understanding [[pdf]](https://openreview.net/forum?id=nYpPAT4L3D)  
(Alibaba, VLM, Medical image interpretation and diagnosis)  
13. Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders [[pdf]](https://openreview.net/forum?id=Y2RW9EVwhT)  
(Nvidia, VLM, MoE, Systematic comparison)
14. PerturboLLaVA: Reducing Multimodal Hallucinations with Perturbative Visual Training [[pdf]](https://openreview.net/forum?id=j4LITBSUjs)  
(ZJU, VLM hallucination, Incorporating adversarially perturbed text during training)  
15. VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks  [[pdf]](https://openreview.net/forum?id=TE0KOzWYAF)  
(U of Waterloo, Universal multimodal embedding, CLIP)


## NAACL-202504
[All Papers](https://aclanthology.org/events/naacl-2025/)

<!--stackedit_data:
eyJoaXN0b3J5IjpbODgwMjYxNjUxLDE3Njc0NzA0NTAsLTQwNT
M3MDMwNiwxMTg0OTc1ODAxLDk5ODEwOTMxMywxNTM2NzAwMDkz
LDE2NTIxODU0NjksMTgzOTkxMDEwMCwyMDQxNTg4ODY3LC0xMT
gwNTk5MDY4LDIxNDI3Njg3NCwtMTYzMjUwNDA1NCwzMTcyNDA3
ODYsLTgxMTQxODUwNSwtMTM3ODcwNzM4OCw2MzE4NTIzNjMsLT
E1OTc5MzA0NTgsLTE5NTE5NTg3MjEsLTEwNDEwODQzODEsLTcz
OTQ1NzM4OV19
-->