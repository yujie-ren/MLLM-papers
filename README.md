## Vision Language Model Papers
Search Keywords: Vision, Visual, Multimodal

## Contents
- 2025
  - [CVPR-202506](#cvpr-202506)
  - [ICML-202507](#icml-202507)
  - [ICLR-202504](#iclr-202504)
  - [NAACL-202504](#naacl-202504)

## CVPR-202506
[All Papers](https://openaccess.thecvf.com/CVPR2025?day=all)
检索词：vision language; vision-language; multimodal
1. CALICO: Part-Focused Semantic Co-Segmentation with Large Vision-Language Models  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Nguyen_CALICO_Part-Focused_Semantic_Co-Segmentation_with_Large_Vision-Language_Models_CVPR_2025_paper.html)  
(UIUC, Make VLMs identify common objects across multiple images)  
2. Task Preference Optimization: Improving Multimodal Large Language Models with Vision Task Alignment  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Yan_Task_Preference_Optimization_Improving_Multimodal_Large_Language_Models_with_Vision_CVPR_2025_paper.html)  
(Shanghai AI Lab, A new training method for VLM, TPO)  
3. Words or Vision: Do Vision-Language Models Have Blind Faith in Text?  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Deng_Words_or_Vision_Do_Vision-Language_Models_Have_Blind_Faith_in_CVPR_2025_paper.html)  
(⭐️NUS, Modality conflict, Blind faith in text)  
4. Evaluating Vision-Language Models as Evaluators in Path Planning  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Aghzal_Evaluating_Vision-Language_Models_as_Evaluators_in_Path_Planning_CVPR_2025_paper.html)  
(George Mason U, Benchmark)  
5. Img-Diff: Contrastive Data Synthesis for Multimodal Large Language Models  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Jiao_Img-Diff_Contrastive_Data_Synthesis_for_Multimodal_Large_Language_Models_CVPR_2025_paper.html)  
(SYU, Synthesize training data for VLM, Object replacement)  
6. Stealthy Backdoor Attack in Self-Supervised Learning Vision Encoders for Large Vision Language Models  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Stealthy_Backdoor_Attack_in_Self-Supervised_Learning_Vision_Encoders_for_Large_CVPR_2025_paper.html)  
(UIUC, Backdoor attack in vision encoder for VLM)  
7. ATP-LLaVA: Adaptive Token Pruning for Large Vision Language Models  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Ye_ATP-LLaVA_Adaptive_Token_Pruning_for_Large_Vision_Language_Models_CVPR_2025_paper.html)  
(THU, Layer-wise & instance-wise vision token pruning)  
8. Lifelong Knowledge Editing for Vision Language Models with Low-Rank Mixture-of-Experts  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Lifelong_Knowledge_Editing_for_Vision_Language_Models_with_Low-Rank_Mixture-of-Experts_CVPR_2025_paper.html)  
(ECNU, Model editing for VLM)  
9. What's in the Image? A Deep-Dive into the Vision of Vision Language Models  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Kaduri_Whats_in_the_Image_A_Deep-Dive_into_the_Vision_of_CVPR_2025_paper.html)  
(Weizmann Institute of Science, VLM interpretability, How VLM process visual information)  
10. PARC: A Quantitative Framework Uncovering the Symmetries within Vision Language Models  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Schmalfuss_PARC_A_Quantitative_Framework_Uncovering_the_Symmetries_within_Vision_Language_CVPR_2025_paper.html)  
(U of Stuttgart, Analysing prompt sensitivity of VLMs)  
11. Forensics-Bench: A Comprehensive Forgery Detection Benchmark Suite for Large Vision Language Models  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Forensics-Bench_A_Comprehensive_Forgery_Detection_Benchmark_Suite_for_Large_Vision_CVPR_2025_paper.html)  
(HKU, Benchmark, Assess VLM's capability in forgery detection)  
12. Embodied Scene Understanding for Vision Language Models via MetaVQA  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Embodied_Scene_Understanding_for_Vision_Language_Models_via_MetaVQA_CVPR_2025_paper.html)  
(UCLA, Benchmark, Assess VLM's spatial reasoning and sequential decision-making capabilities)  
13. MotionBench: Benchmarking and Improving Fine-grained Video Motion Understanding for Vision Language Models  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Hong_MotionBench_Benchmarking_and_Improving_Fine-grained_Video_Motion_Understanding_for_Vision_CVPR_2025_paper.html)  
(THU, Benchmarking VLM's fine-grained motion comprehension)  
14. Video-XL: Extra-Long Vision Language Model for Hour-Scale Video Understanding  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding_CVPR_2025_paper.html)  
(SJTU, VLM for video understanding)  
15. SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language Models  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_SPA-VL_A_Comprehensive_Safety_Preference_Alignment_Dataset_for_Vision_Language_CVPR_2025_paper.html)  
(USTC, Safety alignment dataset for VLM)  
16. FastVLM: Efficient Vision Encoding for Vision Language Models  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Vasu_FastVLM_Efficient_Vision_Encoding_for_Vision_Language_Models_CVPR_2025_paper.html)  
(Apple, VLM effiency, Tradeoff between resolution, latency, and accuracy)  
17. HalLoc: Token-level Localization of Hallucinations for Vision Language Models  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Park_HalLoc_Token-level_Localization_of_Hallucinations_for_Vision_Language_Models_CVPR_2025_paper.html)  
(SNU, Dataset, Token-level hallucination localization)  
18. Steering Away from Harm: An Adaptive Approach to Defending Vision Language Model Against Jailbreaks  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Steering_Away_from_Harm_An_Adaptive_Approach_to_Defending_Vision_CVPR_2025_paper.html)  
(UIUC, Defense jailbreak by steering vectors)  
19. VisionZip: Longer is Better but Not Necessary in Vision Language Models  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Yang_VisionZip_Longer_is_Better_but_Not_Necessary_in_Vision_Language_CVPR_2025_paper.html)  
(CUHK, VLM effiency, Reducing visual tokens)  
20. Automated Generation of Challenging Multiple-Choice Questions for Vision Language Model Evaluation  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Automated_Generation_of_Challenging_Multiple-Choice_Questions_for_Vision_Language_Model_CVPR_2025_paper.html)  
(Standford, Agentic framework converting open-ended questions into multiple-choice format)  
21. Seeing the Abstract: Translating the Abstract Language for Vision Language Models  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Talon_Seeing_the_Abstract_Translating_the_Abstract_Language_for_Vision_Language_CVPR_2025_paper.html)  
(Fondazione Bruno Kessler, Abstract-oriented language in VLMs)  
22. FireEdit: Fine-grained Instruction-based Image Editing via Region-aware Vision Language Model  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_FireEdit_Fine-grained_Instruction-based_Image_Editing_via_Region-aware_Vision_Language_Model_CVPR_2025_paper.html)  
(SYU, Instruction-based image editing, VLM+Diffusion model)  
23. VLsI: Verbalized Layers-to-Interactions from Large to Small Vision Language Models  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Lee_VLsI_Verbalized_Layers-to-Interactions_from_Large_to_Small_Vision_Language_Models_CVPR_2025_paper.html)  
(Nvidia, New VLM family, VLM effiency)  
24. TopV: Compatible Token Pruning with Inference Time Optimization for Fast and Low-Memory Multimodal Vision Language Model  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Yang_TopV_Compatible_Token_Pruning_with_Inference_Time_Optimization_for_Fast_CVPR_2025_paper.html)  
(RutgersU, Visual token pruning)
25. Towards Understanding How Knowledge Evolves in Large Vision-Language Models  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Towards_Understanding_How_Knowledge_Evolves_in_Large_Vision-Language_Models_CVPR_2025_paper.html)  
(⭐️⭐️CAS, Knowledge evolving in VLM)  
26. Revisiting Backdoor Attacks against Large Vision-Language Models from Domain Shift  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Liang_Revisiting_Backdoor_Attacks_against_Large_Vision-Language_Models_from_Domain_Shift_CVPR_2025_paper.html)  
(NTU, Backdoor attack for VLM)  
27. Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art_CVPR_2025_paper.html)  
(AI2, Open source Molmo)  
28. OmniDrive: A Holistic Vision-Language Dataset for Autonomous Driving with Counterfactual Reasoning  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Wang_OmniDrive_A_Holistic_Vision-Language_Dataset_for_Autonomous_Driving_with_Counterfactual_CVPR_2025_paper.html)  
(Nvidia, Dataset, Extend VLMs’ 2D understanding and reasoning to 3D)  
29. Your Large Vision-Language Model Only Needs A Few Attention Heads For Visual Grounding  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Kang_Your_Large_Vision-Language_Model_Only_Needs_A_Few_Attention_Heads_CVPR_2025_paper.html)  
(Yonsei U, VLM for visual grounding)  
30. CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_CoT-VLA_Visual_Chain-of-Thought_Reasoning_for_Vision-Language-Action_Models_CVPR_2025_paper.html)  
(NVIDIA, Visual CoT for VLM)  
31. VL2Lite: Task-Specific Knowledge Distillation from Large Vision-Language Models to Lightweight Networks  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Jang_VL2Lite_Task-Specific_Knowledge_Distillation_from_Large_Vision-Language_Models_to_Lightweight_CVPR_2025_paper.html)
(SK Telecom, Distill VLM to light NN for image classification)  
32. Libra-Merging: Importance-redundancy and Pruning-merging Trade-off for Acceleration Plug-in in Large Vision-Language Model  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Libra-Merging_Importance-redundancy_and_Pruning-merging_Trade-off_for_Acceleration_Plug-in_in_Large_CVPR_2025_paper.html)  
(ZJU, VLM effiency, Visual token pruning)  
33. Nullu: Mitigating Object Hallucinations in Large Vision-Language Models via HalluSpace Projection  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Nullu_Mitigating_Object_Hallucinations_in_Large_Vision-Language_Models_via_HalluSpace_CVPR_2025_paper.html)  
(Xi'an Jiaotong, Model editing for reducing VLM object hallucination)  
34. Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/An_Mitigating_Object_Hallucinations_in_Large_Vision-Language_Models_with_Assembly_of_CVPR_2025_paper.html)  
(Xi'an Jiaotong, Image-prompt matching to reduce VLM's object hallucination)  
35. Mitigating Hallucinations in Large Vision-Language Models via DPO: On-Policy Data Hold the Key  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Mitigating_Hallucinations_in_Large_Vision-Language_Models_via_DPO_On-Policy_Data_CVPR_2025_paper.html)  
(CUHK, On-policy DPO for reducing VLM hallucinations)  
36. STING-BEE: Towards Vision-Language Model for Real-World X-ray Baggage Security Inspection  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Velayudhan_STING-BEE_Towards_Vision-Language_Model_for_Real-World_X-ray_Baggage_Security_Inspection_CVPR_2025_paper.html)  
(Khalifa University of Science and Technology, X-ray baggage security dataset)  
37. BOLT: Boost Large Vision-Language Model Without Training for Long-form Video Understanding  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Liu_BOLT_Boost_Large_Vision-Language_Model_Without_Training_for_Long-form_Video_CVPR_2025_paper.html)  
(KAUST, Frame sampling for Video-language model)  
38. MBQ: Modality-Balanced Quantization for Large Vision-Language Models  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Li_MBQ_Modality-Balanced_Quantization_for_Large_Vision-Language_Models_CVPR_2025_paper.html)  
(THU, Quantization for VLM)  
39. Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Florence-VL_Enhancing_Vision-Language_Models_with_Generative_Vision_Encoder_and_Depth-Breadth_CVPR_2025_paper.html)  
(Uo Maryland, New family of VLMs)  
40. Benchmarking Large Vision-Language Models via Directed Scene Graph for Comprehensive Image Captioning  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Lu_Benchmarking_Large_Vision-Language_Models_via_Directed_Scene_Graph_for_Comprehensive_CVPR_2025_paper.html)  
(USTC, Benchmarking VLM for image captioning)  
41. Conical Visual Concentration for Efficient Large Vision-Language Models  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Xing_Conical_Visual_Concentration_for_Efficient_Large_Vision-Language_Models_CVPR_2025_paper.html)  
(USTC, Visual token reduction)  
42. Unveiling the Mist over 3D Vision-Language Understanding: Object-centric Evaluation with Chain-of-Analysis  [[pdf]](https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Unveiling_the_Mist_over_3D_Vision-Language_Understanding_Object-centric_Evaluation_with_CVPR_2025_paper.html)  
(BIGAI, Benchmark for 3D vision-language model)  



[F^3OCUS - Federated Finetuning of Vision-Language Foundation Models with Optimal Client Layer Updating Strategy via Multi-objective Meta-Heuristics](https://openaccess.thecvf.com/content/CVPR2025/html/Saha_F3OCUS_-_Federated_Finetuning_of_Vision-Language_Foundation_Models_with_Optimal_CVPR_2025_paper.html)
vision-language 45/96
检索vision-language和multimodal，从这篇开始





## ICML-202507
[All Papers](https://openreview.net/group?id=ICML.cc/2025/Conference#tab-accept-oral)
#### MLLM
1. EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents  [[pdf]](https://openreview.net/forum?id=DgGF2LEBPS)  
(UIUC, Benchmark, MLLM, Agent)
2. AffectGPT: A New Dataset, Model, and Benchmark for Emotion Understanding with Multimodal Large Language Models  [[pdf]](https://openreview.net/forum?id=xmbdACI0xu)  
(CAS, Benchmark, MLLM, Emotion understanding)  
3. SK-VQA: Synthetic Knowledge Generation at Scale for Training Context-Augmented Multimodal LLMs  [[pdf]](https://openreview.net/forum?id=EVwMw2lVlw)  
(Intel, Benchmark, MLLM, Multimodal RAG)  
4. On Path to Multimodal Generalist: General-Level and General-Bench [[pdf]](https://openreview.net/forum?id=VsJ1K2HV3k)  
(NUS, Multimodal generation, Evaluation framework)  
5. Can MLLMs Reason in Multimodality? EMMA: An Enhanced MultiModal ReAsoning Benchmark  [[pdf]](https://openreview.net/forum?id=v26vwjxOEz)  
(USTC, Benchmark, MLLM Reasoning)  
6. Mitigating Object Hallucination in Large Vision-Language Models via Image-Grounded Guidance  [[pdf]](https://openreview.net/forum?id=w0xYx9CJhY)  
(Cornell U, VLM, Mitigate hallucination)
7. GMAIL: Generative Modality Alignment for generated Image Learning  [[pdf]](https://openreview.net/forum?id=u6xeKVHS6K)  
(CMU, Training VLM with generated images)
8. MODA: MOdular Duplex Attention for Multimodal Perception, Cognition, and Emotion Understanding  [[pdf]](https://openreview.net/forum?id=9hd5WA6QCn)  
(Kuaishou, MLLM, Attention mechanism)  
9. Data-Juicer Sandbox: A Feedback-Driven Suite for Multimodal Data-Model Co-development  [[pdf]](https://openreview.net/forum?id=zIGIvysR1H)  
(Alibaba, Suite, MLLM, Data-model co-development) 
10. Re-ranking Reasoning Context with Tree Search Makes Large Vision-Language Models Stronger  [[pdf]](https://openreview.net/forum?id=DJcEoC9JpQ)  
(Alibaba, VLM, Multimodal RAG)  
11. Improving Zero-Shot Adversarial Robustness in Vision-Language Models by Closed-form Alignment of Adversarial Path Simplices  [[pdf]](https://openreview.net/forum?id=WR0ahlhOoy)  
(NTU, CLIP, Robustness)  
12. HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation  [[pdf]](https://openreview.net/forum?id=WbP2OwMULq)  
(ZJU, Medical LVLM)  
13. Catch Your Emotion: Sharpening Emotion Perception in Multimodal Large Language Models  [[pdf]](https://openreview.net/forum?id=IYOksPHJKT)  
(Wuhan U, Emotional understanding, Improving during inference)  
14. Layer-wise Alignment: Examining Safety Alignment Across Image Encoder Layers in Vision Language Models  [[pdf]](https://openreview.net/forum?id=F1ff8zcjPp)  
(UC Riverside, VLM safety, Image encoder, Layer-wise, LLaVA)
15. When and How Does CLIP Enable Domain and Compositional Generalization?  [[pdf]](https://openreview.net/forum?id=Lktwi30g63)  
(U of Freiburg, CLIP, Training, Generalization)  
16. A Closer Look at Multimodal Representation Collapse  [[pdf]](https://openreview.net/forum?id=Vf9f7eNX6T)  
(Fujitsu Research, Modality collapse)
17. DS-VLM: Diffusion Supervision Vision Language Model  [[pdf]](https://openreview.net/forum?id=NEBa0bs5LR)  
(Xiamen U, VLM architecture, Combine diffusion & VLM)  
18. Unifying 2D and 3D Vision-Language Understanding  [[pdf]](https://openreview.net/forum?id=FcTeo26AfZ)  
(CMU, New VLM architecture for 3D understanding)  
19. OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature Extraction  [[pdf]](https://openreview.net/forum?id=UHF0km7R5M)  
(UCB, New VLA architecture)  
20. Vision-Language Model Selection and Reuse for Downstream Adaptation  [[pdf]](https://openreview.net/forum?id=bvrsrvo0Mt)  
(NJU, Selecting a VLM for a specific task)  
21. Vision-Language Models Create Cross-Modal Task Representations  [[pdf]](https://openreview.net/forum?id=77ziPGdQct)  
(⭐️UCB, VLM interpretability)  
22. Vision-Language Models Create Cross-Modal Task Representations  [[pdf]](https://openreview.net/forum?id=77ziPGdQct)  
(PKU, Visual token reduction for VLM)  
23. ERICT: Enhancing Robustness by Identifying Concept Tokens in Zero-Shot Vision Language Models  [[pdf]](https://openreview.net/forum?id=ZDsbK7Xx4n)  
(ZJU, spurious correlation in VLM)
24. Do Vision-Language Models Really Understand Visual Language?  [[pdf]](https://openreview.net/forum?id=ZPQU4uGMBA)  
(ETH, Evaluate VLMs' ability of diagrams understanding and reasoning)  
25. OmniBal: Towards Fast Instruction-Tuning for Vision-Language Models via Omniverse Computation Balance  [[pdf]](https://openreview.net/forum?id=q6f9Fg9LHe)  
(SJTU, New training method for VLM)  
26. Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging  [[pdf]](https://openreview.net/forum?id=ntCAP6tMoX)  
(⭐️⭐️CityU, Model merging for VLM, 可以参考这篇文章的结构)  
27. Time-VLM: Exploring Multimodal Vision-Language Models for Augmented Time Series Forecasting  [[pdf]](https://openreview.net/forum?id=b5h60xQnzM)  
(HKUST-GZ, VLM for time series forecasting)  
28. 3D Question Answering via only 2D Vision-Language Models  [[pdf]](https://openreview.net/forum?id=IkhJApkJQ3)  
(NTU, VLM for 3D question answering)  
29. PoisonedEye: Knowledge Poisoning Attack on Retrieval-Augmented Generation based Large Vision-Language Models  [[pdf]](https://openreview.net/forum?id=6SIymOqJlc)  
(Xidian U, Attack for VL-RAG in VLM)  
30. Testing the Limits of Fine-Tuning for Improving Visual Cognition in Vision Language Models  [[pdf]](https://openreview.net/forum?id=jSxU7ZGe3B)  
(Helmholtz Munich, Evaluation, Visual Cognition)  
31. Unlocking the Capabilities of Large Vision-Language Models for Generalizable and Explainable Deepfake Detection  [[pdf]](https://openreview.net/forum?id=vDB2oX3Wl3)  
(Jinan U, VLM for deepfake detection)  
32. Can We Predict Performance of Large Models across Vision-Language Tasks?  [[pdf]](https://openreview.net/forum?id=A9XePhy4Z3)  
(Australian National U, Predicting performance of VLM)  
33. TRUST-VLM: Thorough Red-Teaming for Uncovering Safety Threats in Vision-Language Models  [[pdf]](https://openreview.net/forum?id=dc9vh0NlBR)  
(NTU, Attack method for VLM)  
34. Defending LVLMs Against Vision Attacks Through Partial-Perception Supervision  [[pdf]](https://openreview.net/forum?id=C4F42Ho7IM)  
(ZJU, Defence for VLM)  



#### Other
1. Learning dynamics in linear recurrent neural networks  [[pdf]](https://openreview.net/forum?id=KGOcrIWYnx)  
(⭐️ICL, Learning dynamics of RNN, idea for next project)
2. Learning Dynamics in Continual Pre-Training for Large Language Models  [[pdf]](https://openreview.net/forum?id=Vk1rNMl0J1)  
(⭐️UCAS, Learning dynamics of continual pre-training, idea for next project)  
3. Training Dynamics of In-Context Learning in Linear Attention  [[pdf]](https://openreview.net/forum?id=aFNq67ilos)  
(⭐️UCL, Training dynamics, In-context learning, Attention)  




## ICLR-202504
[All Papers](https://openreview.net/group?id=ICLR.cc/2025/Conference#tab-accept-oral) &nbsp;&nbsp;
[Multimodal Model Papers](https://iclr2025.vizhub.ai/?brushed=%255B%255B179.62503051757812%252C18.363710403442383%255D%252C%255B330.3000183105469%252C234.6387176513672%255D%255D)

####  MLLM
1. Mechanistic Interpretability Meets Vision Language Models: Insights and Limitations  [[pdf]](https://d2jud02ci9yv69.cloudfront.net/2025-04-28-vlm-understanding-29/blog/vlm-understanding/)  
(Blog, THU, 全面介绍了Interpretability & VLM)
2. Fine-Tuning Token-Based Large Multimodal Models: What Works, What Doesn’t and What's Next  [[pdf]](https://d2jud02ci9yv69.cloudfront.net/2025-04-28-fine-tuning-token-based-large-multimodal-models-86/blog/fine-tuning-token-based-large-multimodal-models/)  
(Blog, Pengfei Liu from SHJT, 探索了VLM Fine-tuning)
3. Inference Optimal VLMs Need Fewer Visual Tokens and More Parameters  [[pdf]](https://openreview.net/forum?id=6VhDQP7WGX)  
(CMU, VLM, 研究了减少visual token和减少LLM参数的trade-off, 没有对比实验。)
4. Dynamic Multimodal Evaluation with Flexible Complexity by Vision-Language Bootstrapping  [[pdf]](https://openreview.net/forum?id=X1OfiRYCLn)  
(SJTU, VLM, VLM evaluation, data contamination)  
5. MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models  [[pdf]](https://openreview.net/forum?id=HnhNRrLPwm)  
(UNC-Chapel Hill, VLM, VLM evaluation)  
6. Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference under Ambiguities  [[pdf]](https://openreview.net/forum?id=84pDoCD4lH)  
(U of Michigan, VLM evaluation)  
7. PhysBench: Benchmarking and Enhancing Vision-Language Models for Physical World Understanding  [[pdf]](https://openreview.net/forum?id=Q6a9W6kzv5)  
(USC, VLM Evaluation)  
8.  Reducing Hallucinations in Large Vision-Language Models via Latent Space Steering  [[pdf]](https://openreview.net/forum?id=LBl7Hez0fF)  
(Stanford, VLM Hallucination)  
9. Vision Language Models are In-Context Value Learners  [[pdf]](https://openreview.net/forum?id=friHAl5ofG)  
(Google, VLM, Robots)  
10. AHA: A Vision-Language-Model for Detecting and Reasoning Over Failures in Robotic Manipulation  [[pdf]](https://openreview.net/forum?id=JVkdSi7Ekg)  
(Nvidia, VLM, Robot, Detect failures)  
11. Teaching Human Behavior Improves Content Understanding Abilities Of VLMs  [[pdf]](https://openreview.net/forum?id=ff2V3UR9sC)  
(Adobe, VLM, Training VLMs to predict receiver behaviors)  
12. NL-Eye: Abductive NLI For Images  [[pdf]](https://openreview.net/forum?id=2zmO1GVT0Y)  
(Technion, VLM Evaluation, Natural language inference in visual domain)  
13. LLaRA: Supercharging Robot Learning Data for Vision-Language Policy [[pdf]](https://openreview.net/forum?id=iVxxgZlXh6)  
(Stony Brook U, VLA model, Motivated by visual instruction tuning)  
14. Failures to Find Transferable Image Jailbreaks Between Vision-Language Models [[pdf]](https://openreview.net/forum?id=wvFnqVVUhN)  
(Stanford, VLM, Image jailbreak)
15. Have the VLMs Lost Confidence? A Study of Sycophancy in VLMs [[pdf]](https://openreview.net/forum?id=E2PFv7ad3p)  
(Fudan U, Sycophancy, Hallucination)  
16. Is Your Video Language Model a Reliable Judge? [[pdf]](https://openreview.net/forum?id=m8yby1JfbU)  
(Iowa State U, Video language models, VLM as a judge)  
17. LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language Models for Referring Expression Comprehension [[pdf]](https://openreview.net/forum?id=PgXpOOqtyd)  
(Valeo.ai from France, VLM, Referring Expression Comprehension)  
18. Backdooring Vision-Language Models with Out-Of-Distribution Data [[pdf]](https://openreview.net/forum?id=tZozeR3VV7)  
(Stony Brook U, VLM, Backdoor attack)  
19. Do Vision & Language Decoders use Images and Text equally? How Self-consistent are their Explanations? [[pdf]](https://openreview.net/forum?id=lCasyP21Bf)  
(⭐️Heidelberg U, Multimodal degree of a VLM)  
20. VL-Cache: Sparsity and Modality-Aware KV Cache Compression for Vision-Language Model Inference Acceleration [[pdf]](https://openreview.net/forum?id=HMrcv7Q4Ub)  
(⭐️UCLA, KV cache in VLM)
21. Benchmarking Vision Language Model Unlearning via Fictitious Facial Identity Dataset [[pdf]](https://openreview.net/forum?id=0y3hGn1wOk)  
(UWM, VLM unlearning benchmark)  
22. Re-Aligning Language to Visual Objects with an Agentic Workflow [[pdf]](https://openreview.net/forum?id=MPJ4SMnScw)  
(Nankai U, Language-based object detection, Agentic workflow, VLM hallucination)  
23. BlueSuffix: Reinforced Blue Teaming for Vision-Language Models Against Jailbreak Attacks [[pdf]](https://openreview.net/forum?id=wwVGZRnAYG)  
(Fudan U, Black-box defense for VLMs against jailbreak attacks)  
24. ImagineNav: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination [[pdf]](https://openreview.net/forum?id=vQFw9ryKyK)  
(Southeast U, Visual navigation, Robots)  
25. Should VLMs be Pre-trained with Image Data? [[pdf]](https://openreview.net/forum?id=Pj4Aid3XqL)  
(⭐️Toyota Research Institute, Pre-train with image or not)  
26. Can We Talk Models Into Seeing the World Differently? [[pdf]](https://openreview.net/forum?id=iVMcYxTiVM)  
(Offenburg U, Vision biases,Shape/texture bias)  
27. Towards Interpreting Visual Information Processing in Vision-Language Models [[pdf]](https://openreview.net/forum?id=chanJGoa7f)  
(NTU, VLM interpretability)  
28. Painting with Words: Elevating Detailed Image Captioning with Benchmark and Alignment Learning [[pdf]](https://openreview.net/forum?id=636M0nNbPs)  
(ByteDance, Image captioning evaluation)  
29. IDA-VLM: Towards Movie Understanding via ID-Aware Large Vision-Language Model [[pdf]](https://openreview.net/forum?id=N5YTixK4F1)  
(UHK, Movie understanding, Associate instances across different scenes)  
30. HAMSTER: Hierarchical Action Models for Open-World Robot Manipulation [[pdf]](https://openreview.net/forum?id=h7aQxzKbq6)  
(Nvidia, Train VLM with off-domain data)  
31. Locality Alignment Improves Vision-Language Models [[pdf]](https://openreview.net/forum?id=qssVptHTPN)  
(Stanford, VLM spatial reasoning, Train with locality alignment)  
32. CogCoM: A Visual Language Model with Chain-of-Manipulations Reasoning [[pdf]](https://openreview.net/forum?id=Fg0eo2AkST)  
(THU, Chain of Manipulations)  
33. Causal Graphical Models for Vision-Language Compositional Understanding [[pdf]](https://openreview.net/forum?id=haJHr4UsQX)  
(University of Modena and Reggio Emilia from Italy, Compositional tasks)  
34. Probabilistic Language-Image Pre-Training [[pdf]](https://openreview.net/forum?id=D5X6nPGFUY)  
(NAVER AI from Korea, CLIP, Probabilistic pretraining)  
35. RA-TTA: Retrieval-Augmented Test-Time Adaptation for Vision-Language Models [[pdf]](https://openreview.net/forum?id=V3zobHnS61)  
(KAIST, Retrieval-augmented test-time adaptation)  
36. Mitigating Spurious Correlations in Zero-Shot Multimodal Models [[pdf]](https://openreview.net/forum?id=UsRKFYR4lM)  
(Purdue U, CLIP, Spurious correlation)  
37. DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models [[pdf]](https://openreview.net/forum?id=VOAMTA8jKu)  
(UIUC, Mathematical reasoning robustness benchmarking)  
38. See It from My Perspective: How Language Affects Cultural Bias in Image Understanding [[pdf]](https://openreview.net/forum?id=Xbl6t6zxZs)  
(Columbia U, Cutural bais in image understanding)  
39. DAMO: Decoding by Accumulating Activations Momentum for Mitigating Hallucinations in Vision-Language Models [[pdf]](https://openreview.net/forum?id=JUr0YOMvZA)  
(U of Maryland, Hallucination, Layer-wise analysis)  
40. Chain-of-region: Visual Language Models Need Details for Diagram Analysis [[pdf]](https://openreview.net/forum?id=M6fYrICcQs)  
(UWM, Visual details, Region decomposion, Scientific diagram analysis)  
41. Mixture of Experts Made Personalized: Federated Prompt Learning for Vision-Language Models [[pdf]](https://openreview.net/forum?id=xiDJaTim3P)  
(U of Pittsburgh, CLIP, Federated learning)  
42. VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents [[pdf]](https://openreview.net/forum?id=zG459X3Xge)  
(THU, VLM-based RAG)  
43. Generating CAD Code with Vision-Language Models for 3D Designs [[pdf]](https://openreview.net/forum?id=BLWaTeucYX)  
(Georgia IT, CAD code verification)  
44. Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations [[pdf]](https://openreview.net/forum?id=94kQgWXojH)  
(⭐️UC Berkely, VLM, Interpretation, Hallucination)  
45. VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation [[pdf]](https://openreview.net/forum?id=02haSpO453)  
(THU, Unified model, Video, Image, Language understanding and generation)  
46. Enhancing Cognition and Explainability of Multimodal Foundation Models with Self-Synthesized Data [[pdf]](https://openreview.net/forum?id=lHbLpwbEyt)  
(U of Georgia, Fine-grained visual reasoning)  
47. Modality-Specialized Synergizers for Interleaved Vision-Language Generalists [[pdf]](https://openreview.net/forum?id=7UgQjFEadn)  
(Virginia Tech, Vision-Language Generalists, Understanding and generating both text and images)  
48. How Does Vision-Language Adaptation Impact the Safety of Vision Language Models? [[pdf]](https://openreview.net/forum?id=eXB5TCrAu9)  
(⭐️⭐️KAIST, Vision language adaptation, Safety, Model merging)  
49. Cross-Modal Safety Mechanism Transfer in Large Vision-Language Models [[pdf]](https://openreview.net/forum?id=45rvZkJbuX)  
(⭐️⭐️CAS, Vision-language alignment, Safety transfer)



#### Other
1. Knowledge Entropy Decay during Language Model Pretraining Hinders New Knowledge Acquisition  [[pdf]](https://openreview.net/forum?id=eHehzSDUFp)  
(KAIST AI, Pretraining过程中参数化知识的整合倾向)
2.  Context-Parametric Inversion: Why Instruction Finetuning May Not Actually Improve Context Reliance  [[pdf]](https://openreview.net/forum?id=SPS6HzVzyt)  
(CMU，知识冲突中的上下文依赖)
3.  Scaling LLM Test-Time Compute Optimally Can be More Effective than Scaling Parameters for Reasoning  [[pdf]](https://openreview.net/forum?id=4FWAwZtd2n)  
(UC Berkeley，test time computing)
4.  Capturing the Temporal Dependence of Training Data Influence  [[pdf]](https://openreview.net/forum?id=uHLgDEgiS5)  
(Princeton, the influence of specific training data)
5. Data Shapley in One Training Run  [[pdf]](https://openreview.net/forum?id=HD6bWcj87Y)  
(Princeton, the influence of specific training data)
6.  Interpreting Emergent Planning in Model-Free Reinforcement Learning  [[pdf]](https://openreview.net/forum?id=DzGe40glxs)  
(Cambridge, Interpretability, RL agents)  
7.  Unlocking the Power of Function Vectors for Characterizing and Mitigating Catastrophic Forgetting in Continual Instruction Tuning  [[pdf]](https://openreview.net/forum?id=gc8QAQfXv6)  
(USTC, Catastrophic forgetting, Continual learning)  
8. Two Effects, One Trigger: On the Modality Gap, Object Bias, and Information Imbalance in Contrastive Vision-Language Models  [[pdf]](https://openreview.net/forum?id=uAFHCZRmXk)  
(U of Freiburg, CLIP)  
9. Weighted Point Set Embedding for Multimodal Contrastive Learning Toward Optimal Similarity Metric  [[pdf]](https://openreview.net/forum?id=uSz2K30RRd)  
(Sony AI, CLIP)  
10. VLMaterial: Procedural Material Generation with Large Vision-Language Models  [[pdf]](https://openreview.net/forum?id=wHebuIb6IH)  
(MIT, VLM, Computer graphics)  
11. LLaVA-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models  [[pdf]](https://openreview.net/forum?id=oSQiao9GqB)  
(ByteDance, VLM, Multi-image, Video, 3D)  
12. Large-scale and Fine-grained Vision-language Pre-training for Enhanced CT Image Understanding [[pdf]](https://openreview.net/forum?id=nYpPAT4L3D)  
(Alibaba, VLM, Medical image interpretation and diagnosis)  
13. Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders [[pdf]](https://openreview.net/forum?id=Y2RW9EVwhT)  
(Nvidia, VLM, MoE, Systematic comparison)
14. PerturboLLaVA: Reducing Multimodal Hallucinations with Perturbative Visual Training [[pdf]](https://openreview.net/forum?id=j4LITBSUjs)  
(ZJU, VLM hallucination, Incorporating adversarially perturbed text during training)  
15. VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks  [[pdf]](https://openreview.net/forum?id=TE0KOzWYAF)  
(U of Waterloo, Universal multimodal embedding, CLIP)


## NAACL-202504
[All Papers](https://aclanthology.org/events/naacl-2025/)

1. Uncovering Bias in Large Vision-Language Models at Scale with Counterfactuals  [[pdf]](https://aclanthology.org/2025.naacl-long.305/)  
(Intel, Social bias in VLM)  
2. ImgTrojan: Jailbreaking Vision-Language Models with  ONE  Image  [[pdf]](https://aclanthology.org/2025.naacl-long.360/)  
(HKU, Jailbreak VLM)  
3. Vision-Language Models Can Self-Improve Reasoning via Reflection  [[pdf]](https://aclanthology.org/2025.naacl-long.447/)  
(NJU, VLM reasoning)  
4. Enhancing Visual-Language Modality Alignment in Large Vision Language Models via Self-Improvement  [[pdf]](https://aclanthology.org/2025.findings-naacl.15/)  
(UoMaryland, New visual instruction tuning method)  
5. VLind-Bench: Measuring Language Priors in Large Vision-Language Models  [[pdf]](https://aclanthology.org/2025.findings-naacl.231/)  
(Seoul National U, Benchmark, Language prior)  
6. Mitigating Hallucinations in Large Vision-Language Models via Summary-Guided Decoding  [[pdf]](https://aclanthology.org/2025.findings-naacl.235/)  
(Seoul National U, Hallucination due to language prior)  
7. LVPruning: An Effective yet Simple Language-Guided Vision Token Pruning Approach for Multi-modal Large Language Models  [[pdf]](https://aclanthology.org/2025.findings-naacl.242/)  
(U of Manchester, Vision token pruning)  
8. How do Multimodal Foundation Models Encode Text and Speech? An Analysis of Cross-Lingual and Cross-Modal Representations  [[pdf]](https://aclanthology.org/2025.naacl-short.51/)  
(KIT, VLM interpretability, Internal representation)  

<!--stackedit_data:
eyJoaXN0b3J5IjpbLTU2NDExNjQ0MCwxOTc1NTIzNjcwLDcyND
YyMzg4OSwyMDY5MjU0NTI5LC04MTI5MzI5ODUsNDQ4MzU1NDI0
LC0xNTIzNjgxMzgxLDgzOTc5Njc0MSwxNDM3NjkxMzMyLC02Mj
UzMjI4MTMsMTM1MjA4MjYxMCwtMTAyMTY0NDk3NCwtMzk2MTQw
MjQ3LDE3NDY5NTI1NjQsMjEwMzEyMzMxLDUzMTE3NDIxMSwtND
A5OTEzODEwLC0xNjkyMTM3MzMsLTExNzcyNTYzOCwtOTQ0Mjgz
NTg2XX0=
-->